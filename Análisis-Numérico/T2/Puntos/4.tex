%!TEX root = ../main.tex

\subsection*{Ejercicio 4.}
Al aproximar una función continua $f : [0, 1] \to \mathbb{R}$ mediante un polinomio $p(t) = a_n t^n + \cdots + a_1 t + a_0$, el error de aproximación $E$ se mide en la norma $L^2$, es decir:
\[
E^2 := \|p - f\|_{L^2}^2 = \int_0^1 [p(t) - f(t)]^2 \, dt.
\]

\begin{enumerate}
    \item[(a)] Muestre que la minimización del error $E = E(a_0, \ldots, a_n)$ conduce a un sistema de ecuaciones lineales $H_n a = b$, donde:
    \[
    b = [b_0, \ldots, b_n]^T \in \mathbb{R}^{n+1}, \quad b_i = \int_0^1 f(t)t^i \, dt, \quad i = 0, \ldots, n,
    \]
    y $H_n$ es la matriz de Hilbert de orden $n$, definida como:
    \[
    (H_n)_{i,j} = \frac{1}{i + j + 1}, \quad i, j = 0, \ldots, n.
    \]
    El vector $a$ representa los coeficientes del polinomio $p$.\\

    \begin{proof}
    Sea $p(t)=\displaystyle\sum_{j=0}^{n} a_jt^j$, note que 

    \begin{align*}
        E^2(a_0,\ldots,a_n)&=\int_0^1\left(p(t)-f(t)\right)^2dt\\
        &=\int_0^1\left(\sum_{j=0}^{n} a_jt^j-f(t)\right)^2dt\\
        &=\int_0^1\left(\sum_{j=0}^{n} a_jt^j\right)^2dt-2\int_0^1f(t)\sum_{j=0}^{n} a_jt^j dt+\|f\|_{L^2}^2
    .\end{align*}

    Note que esta función depende de los coneficientes, por lo tanto para minimizar el error podemos derivar parcialmente con respecto a $a_k$, $0\leq k\leq n$ e igualar a 0. Derivando obtenemos que

    \begin{align*}
        \frac{\partial E^2}{\partial a_k}=2\int_0^1\left(\sum_{j=0}^{n} a_jt^{k+j}\right)dt-2\int_0^1f(t)t^kdt=0 
    ,\end{align*}
    
    despejando de esta ecuación obtenemos que

    $$\int_0^1\sum_{j=0}^{n} a_jt^{j+k}dt=\sum_{j=0}^{n} a_j\int_0^1t^{j+k}dt=\int_0^1f(t)t^k dt,$$

    más aún, como $\displaystyle\int_0^1t^{k+j}dt=\frac{1}{k+j+1}=(H_n)_{k,j}$, obtenemos

    $$\sum_{j=0}^{n} a_j(H_n)_{k,j}=(H_n)_k\begin{pmatrix}
           a_{1} \\
           a_{2} \\
           \vdots \\
           a_{n}
         \end{pmatrix}=\int_0^1f(t)t^kdt=b_k,$$

donde $(H_n)_k$ denota la $k-$ésima fila de la matriz $(H_n)$, que es lo mismo que $H_n a=b$.


    \end{proof}

    \item[(b)] Muestre que $H_n$ es simétrica y definida positiva.\\

    \begin{proof}
    Note que la simetría es inmediata de  que

    $$(H_n)_{i,j}=\frac{1}{i+j+1}=\frac{1}{j+i+1}=(H_n)_{j,i}.$$

    Para ver que la matriz es definida positiva note que
    $$
\begin{aligned}
x^T H_n x & =\left(x_1, \ldots, x_n\right)\left(\begin{array}{ccc}
1 & \cdots & \dfrac{1}{n+2} \\
\vdots & \ddots & \vdots \\
\dfrac{1}{n+2} & \cdots & \dfrac{1}{2 n+1}
\end{array}\right)\left(\begin{array}{c}
x_1 \\
\vdots \\
x_n
\end{array}\right) \\
& =\sum_{i=0}^n \sum_{j=0}^n \frac{x_j x_i}{i+j+1} \\
& =\sum_{i=0}^n \sum_{j=0}^n x_j x_i \int_0^1 t^i t^j d t \\
& =\int_0^1 \sum_{i=0}^n x_i t^i \sum_{j=0}^n x_j t^j d t \\
& =\left\|\sum_{j=0}^{n} x_jt^j\right\|_{L^2}^2>0
\end{aligned}
$$
    \end{proof}

    \item[(c)] Solucione el sistema $H_n x = b$, donde $b$ tiene componentes $b_i = 1 / (n + i - 1)$, para $i = 1, \ldots, n$. Para esto, use las factorizaciones LU ($[L, U] = \text{lu}(H)$) y Cholesky ($L = \text{chol}(H)$). Luego resuelva los dos sistemas triangulares.
    \item[(d)] Para ambos métodos, ¿qué tan precisas son las soluciones numéricas $\hat{x}_{\text{approx}}$? Tabule los errores de la solución:
    \[
    e(n) = \|\hat{x}_{\text{approx}} - x_{\text{exact}}\|
    \]
    como una función de $n = 2, \ldots, 15$. Note que $x_{\text{exact}} = (0, \ldots, 1)^T$. Puede graficar los errores en función de $n$ utilizando la función \texttt{semilogy} de Matlab. Explique en detalle los resultados.
\end{enumerate}

